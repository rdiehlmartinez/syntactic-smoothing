# Syntactic Smoothing

Language models are typically not trained to directly encode syntactic information. In this project, we construct a smoothing approach that can be added to the likelihood maximization objective that encodes a syntactic linguistic prior. We find that using this training approach also reduces the amount of anisotropy in the model's representations. 

## Setup 

To get setup create a hugging face account and ask @rdiehlmartinez to add you to the group's private hugging face hub. The hub is where we keep data, tokenization, model and other artifacts. During training, we pull in these values directly from the hub (and occasionally also push progamatically to the hub). 

In order to interact with the hub, you need to generate read and write [access tokens](https://huggingface.co/docs/hub/security-tokens) from your hugging face account. Once generated, you should store the value for the write tokens as HF_WRITE_TOKEN in a file called `.env`.

You will also need to ask @rdiehlmartinez to add you to the wandb (weights and biases) project. We use wandb to log out metrics generated by our runs. Once you've joined the group, you will need to go to wandb to retrieve your [API key](https://wandb.ai/authorize). You will be prompted for this key calling the `./setup.sh` (see below).

Before running the code, make sure to run the setup script `./setup.sh`. This script sets up the requirements imports as well as git hooks for automatic code formatting. Additionally, this script makes sure you are logged into wandb and huggingface.

## Overview 

The entry point to the codebase is the `train.py` file. This file expects to receive a hydra-style config file that stores all relevant parameters for the dataset, data processing, tokenization, and model training. [Hydra](https://hydra.cc/docs/tutorials/structured_config/intro/) provides a system for structuring config files in a hierarchical, decomposable format.

In the subsequent section, we outline the high-level structure of our code-base. 

### Config Files
Under `/src/config.py` you will find the general structure of the hydra config file that our program expects. The purpose of explicitly defining the structure of the config in this manner is two fold 1) to show the user the set of available configurable options 2) to run type-checking on passed in configs, ensuring that the parameters and their types match this pre-defined format. 

We run automatic type-checking on all the passed in config files, and also check that there are no missing required parameters of the config file. If there are, we raise an error.

The `/conf` directory stores all the default configs and subconfigs. The entry point to the default config we use is `conf/config.yaml`. Taking a look at the `conf` directory, you will notice that each sub-directory of `conf` (i.e. `conf/data_curriculum`) stores a sub-configuration. 

#### Specifying the Objective Task 
The main logic for specifying objective tasks is found within `src/objective_task`, which contains most of the custom logic for pos-smoothing. 

### Preprocessing and Tokenization

Other useful methods for data preprocessing, tokenizer and inference can be found under `src/utils`.

### Evaluation

The evaluation of the model on GLUE and BLIMP tasks is done by calling on third-party libraries that are part of the submodules of this project.

### Model Architecture 

For most of our experiments, we use variants of Roberta language models. The architectures and the associated configurations are specified under `/src/models`. To associate a model name with a given huggingface model and an assocaited config, we store a registry inside of the `models` package. When we load a model we query this registry. 

### Analysis

The anisotropy analyses we conduct are done in jupyter notebooks found under the `notebooks/...` folder.
