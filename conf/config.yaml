defaults: 
  - base_config
  - _self_
  - dataset: uncleaned_strict_small_gold
  - objective_curriculum: base_mlm
  - model: roberta_pln_small
  - tokenizer: cbt_8192

dataset: 
  name: 'CamBabyTrainers/BabyLM'

model:
  model_kwargs:
    vocab_size: 8192

experiment: 
  seed: 42 

data_preprocessing:
  include_punctuation: True
  join_sentences: True
  max_input_length: 128

trainer: 
  batch_size: 512 
  lr: 2e-3 # 1e-4 is used in fairseq; 1e-3 is default in huggingface
  num_warmup_steps: 50_000
  max_training_steps: 200_000
  evaluation_metrics: # One or more of 'blimp', 'blimp_bias', 'perplexity', 'aoa', 'glue' and 'msgs'
    - blimp
    - blimp_bias
    - perplexity

pos_lookup:
  similarity_metric: 'cosine'