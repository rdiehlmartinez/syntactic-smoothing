{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rd654/.cache/pypoetry/virtualenvs/pos-merge-Cf-JYAyy-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append('..')\n",
    "import re\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.core.config_store import ConfigStore\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.training_args import TrainingArguments\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaPreLayerNormForMaskedLM, AutoConfig\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from src.trainer import CustomTrainer\n",
    "from src.utils.data import DatasetPreprocessor \n",
    "\n",
    "from src.config import BabyLMConfig\n",
    "from src.models import load_base_model\n",
    "from src.tokenizer import load_tokenizer\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data utils and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_collate_fn(_samples):\n",
    "    joined_batch = defaultdict(list)\n",
    "    for sample in _samples:\n",
    "        for key, val in sample.items():\n",
    "            joined_batch[key].append(torch.tensor(val))\n",
    "\n",
    "    batch = {}\n",
    "\n",
    "    for key, val in joined_batch.items():\n",
    "        batch[key] = torch.stack(val)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnisotropyComputation: \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers=8,\n",
    "        layer_name_template=re.compile(\"roberta_prelayernorm.encoder.layer.(\\d+).output$\")\n",
    "    ):\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_name_template = layer_name_template\n",
    "\n",
    "        self.cosine_sims = defaultdict(list)\n",
    "    \n",
    "    def get_fw_hook(self, module_name):\n",
    "        def _forward_hook(module, _, module_out):\n",
    "            # compute the cosine similarity between two random vectors in module_output\n",
    "            # between first and second element of batch\n",
    "            indices = torch.randint(0, 128, (2,))\n",
    "            selected_vectors = torch.stack([module_out[i, index] for i, index in enumerate(indices)])\n",
    "            cosine_sim = F.cosine_similarity(selected_vectors[0].unsqueeze(0), selected_vectors[1].unsqueeze(0))\n",
    "\n",
    "            self.cosine_sims[module_name].append(cosine_sim.item())\n",
    "       \n",
    "        return _forward_hook\n",
    "\n",
    "    def setup_hooks(self, model):\n",
    "        for name, module in model.named_modules():\n",
    "            if self.layer_name_template.match(name):\n",
    "                module.register_forward_hook(self.get_fw_hook(name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = ConfigStore.instance()\n",
    "cs.store(name=\"base_config\", node=BabyLMConfig)\n",
    "\n",
    "\n",
    "baseline_model_path = \"/home/rd654/pos-merge/models/pos_merge-roberta_pre_layer_norm-model/lm_model\"\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../conf\",):\n",
    "    cfg = compose(\n",
    "        config_name=\"config\",\n",
    "        overrides=[\n",
    "            \"experiment.name=baseline_clean\",\n",
    "            \"experiment.group=anisotropy\",\n",
    "            \"dataset=strict_small_gold\",\n",
    "            \"experiment.offline_run=True\",\n",
    "        ])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    cfg.dataset.name,\n",
    "    cfg.dataset.subconfig,\n",
    ")  # type: ignore\n",
    "tokenizer = load_tokenizer(cfg)\n",
    "data_preprocessor = DatasetPreprocessor(cfg, tokenizer)\n",
    "eval_dataset = dataset[\"validation\"].map(\n",
    "    data_preprocessor,\n",
    "    batched=True,\n",
    "    num_proc=64,\n",
    "    remove_columns=dataset[\"validation\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model Anisotropy Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaPreLayerNormForMaskedLM(\n",
       "  (roberta_prelayernorm): RobertaPreLayerNormModel(\n",
       "    (embeddings): RobertaPreLayerNormEmbeddings(\n",
       "      (word_embeddings): Embedding(8192, 256, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 256, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaPreLayerNormEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): RobertaPreLayerNormLMHead(\n",
       "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=256, out_features=8192, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model_config = AutoConfig.from_pretrained(baseline_model_path)\n",
    "baseline_model = RobertaPreLayerNormForMaskedLM.from_pretrained(baseline_model_path)\n",
    "baseline_model.cuda()\n",
    "baseline_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random shuffle eval_dataset\n",
    "eval_dataset = eval_dataset.shuffle(seed=42)\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size = 2,\n",
    "    collate_fn=base_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_anisotropy_computation = AnisotropyComputation()\n",
    "baseline_anisotropy_computation.setup_hooks(baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53277/53277 [13:54<00:00, 63.82it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, _batch in tqdm(enumerate(eval_dataloader)):\n",
    "    if batch_idx > 1000:\n",
    "        break\n",
    "\n",
    "    batch  = {\n",
    "        \"input_ids\": _batch[\"input_ids\"].to(\"cuda\"),\n",
    "        \"attention_mask\": _batch[\"attention_mask\"].to(\"cuda\"),\n",
    "    }\n",
    "    baseline_model_output = baseline_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE RESULTS\n",
      "Layer: roberta_prelayernorm.encoder.layer.0.output\n",
      "Label smoothing avg cosine sim: 0.3433568946595581\n",
      "Layer: roberta_prelayernorm.encoder.layer.1.output\n",
      "Label smoothing avg cosine sim: 0.5185323326796372\n",
      "Layer: roberta_prelayernorm.encoder.layer.2.output\n",
      "Label smoothing avg cosine sim: 0.5281436800235679\n",
      "Layer: roberta_prelayernorm.encoder.layer.3.output\n",
      "Label smoothing avg cosine sim: 0.5132462359630755\n",
      "Layer: roberta_prelayernorm.encoder.layer.4.output\n",
      "Label smoothing avg cosine sim: 0.519649225508813\n",
      "Layer: roberta_prelayernorm.encoder.layer.5.output\n",
      "Label smoothing avg cosine sim: 0.5047696811129256\n",
      "Layer: roberta_prelayernorm.encoder.layer.6.output\n",
      "Label smoothing avg cosine sim: 0.5201953046251375\n",
      "Layer: roberta_prelayernorm.encoder.layer.7.output\n",
      "Label smoothing avg cosine sim: 0.4949028764975487\n"
     ]
    }
   ],
   "source": [
    "print(\"BASELINE RESULTS\")\n",
    "for layer_name in baseline_anisotropy_computation.cosine_sims.keys():\n",
    "    # average the cosine similarities\n",
    "    baseline_cosine_sims = baseline_anisotropy_computation.cosine_sims[layer_name]\n",
    "    baseline_avg_cosine_sim = sum(baseline_cosine_sims) / len(baseline_cosine_sims)\n",
    "\n",
    "    print(f\"Layer: {layer_name}\")\n",
    "    print(f\"Label smoothing avg cosine sim: {baseline_avg_cosine_sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Smoothing Model Anisotropy Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_smoothing_model_path = \"/home/rd654/pos-merge/models/tied_cosine_lin_08_08/lm_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaPreLayerNormForMaskedLM(\n",
       "  (roberta_prelayernorm): RobertaPreLayerNormModel(\n",
       "    (embeddings): RobertaPreLayerNormEmbeddings(\n",
       "      (word_embeddings): Embedding(8192, 256, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 256, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaPreLayerNormEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): RobertaPreLayerNormLMHead(\n",
       "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=256, out_features=8192, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_smoothing_model_config = AutoConfig.from_pretrained(pos_smoothing_model_path)\n",
    "pos_smoothing_model = RobertaPreLayerNormForMaskedLM.from_pretrained(pos_smoothing_model_path)\n",
    "pos_smoothing_model.cuda()\n",
    "pos_smoothing_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = eval_dataset.shuffle(seed=42)\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size = 2,\n",
    "    collate_fn=base_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_smoothing_anisotropy_computation = AnisotropyComputation()\n",
    "pos_smoothing_anisotropy_computation.setup_hooks(pos_smoothing_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [00:18, 55.24it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, _batch in tqdm(enumerate(eval_dataloader)):\n",
    "\n",
    "    if idx > 1_000:\n",
    "        break \n",
    "\n",
    "    batch  = {\n",
    "        \"input_ids\": _batch[\"input_ids\"].to(\"cuda\"),\n",
    "        \"attention_mask\": _batch[\"attention_mask\"].to(\"cuda\"),\n",
    "    }\n",
    "    pos_smoothing_model_output = pos_smoothing_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS SMOOTHING RESULTS\n",
      "Layer: roberta_prelayernorm.encoder.layer.0.output\n",
      "Label smoothing avg cosine sim: 0.2706220190060945\n",
      "Layer: roberta_prelayernorm.encoder.layer.1.output\n",
      "Label smoothing avg cosine sim: 0.3794020617363991\n",
      "Layer: roberta_prelayernorm.encoder.layer.2.output\n",
      "Label smoothing avg cosine sim: 0.3808936393567613\n",
      "Layer: roberta_prelayernorm.encoder.layer.3.output\n",
      "Label smoothing avg cosine sim: 0.3289081412990886\n",
      "Layer: roberta_prelayernorm.encoder.layer.4.output\n",
      "Label smoothing avg cosine sim: 0.3371795227231977\n",
      "Layer: roberta_prelayernorm.encoder.layer.5.output\n",
      "Label smoothing avg cosine sim: 0.37615031983468916\n",
      "Layer: roberta_prelayernorm.encoder.layer.6.output\n",
      "Label smoothing avg cosine sim: 0.3598732343994892\n",
      "Layer: roberta_prelayernorm.encoder.layer.7.output\n",
      "Label smoothing avg cosine sim: 0.28189891604690365\n"
     ]
    }
   ],
   "source": [
    "print(\"POS SMOOTHING RESULTS\")\n",
    "for layer_name in pos_smoothing_anisotropy_computation.cosine_sims.keys():\n",
    "    # average the cosine similarities\n",
    "    pos_smoothing_cosine_sims = pos_smoothing_anisotropy_computation.cosine_sims[layer_name]\n",
    "    pos_smoothing_avg_cosine_sim = sum(pos_smoothing_cosine_sims) / len(pos_smoothing_cosine_sims)\n",
    "\n",
    "    print(f\"Layer: {layer_name}\")\n",
    "    print(f\"Label smoothing avg cosine sim: {pos_smoothing_avg_cosine_sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Smoothing Model Anisotropy Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smoothing_model_path = \"/home/rd654/pos-merge/models/tied_baseline_label_smoothing/lm_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaPreLayerNormForMaskedLM(\n",
       "  (roberta_prelayernorm): RobertaPreLayerNormModel(\n",
       "    (embeddings): RobertaPreLayerNormEmbeddings(\n",
       "      (word_embeddings): Embedding(8192, 256, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 256, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaPreLayerNormEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaPreLayerNormLayer(\n",
       "          (attention): RobertaPreLayerNormAttention(\n",
       "            (self): RobertaPreLayerNormSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaPreLayerNormSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): RobertaPreLayerNormIntermediate(\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaPreLayerNormOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): RobertaPreLayerNormLMHead(\n",
       "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=256, out_features=8192, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_smoothing_model_config = AutoConfig.from_pretrained(label_smoothing_model_path)\n",
    "label_smoothing_model = RobertaPreLayerNormForMaskedLM.from_pretrained(label_smoothing_model_path)\n",
    "label_smoothing_model.cuda()\n",
    "label_smoothing_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random shuffle eval_dataset\n",
    "eval_dataset = eval_dataset.shuffle(seed=42)\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size = 2,\n",
    "    collate_fn=base_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smoothing_anisotropy_computation = AnisotropyComputation()\n",
    "label_smoothing_anisotropy_computation.setup_hooks(label_smoothing_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [00:17, 58.34it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, _batch in tqdm(enumerate(eval_dataloader)):\n",
    "\n",
    "    if idx > 1_000:\n",
    "        break \n",
    "\n",
    "    batch  = {\n",
    "        \"input_ids\": _batch[\"input_ids\"].to(\"cuda\"),\n",
    "        \"attention_mask\": _batch[\"attention_mask\"].to(\"cuda\"),\n",
    "    }\n",
    "    label_smoothing_model_output = label_smoothing_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL SMOOTHING RESULTS\n",
      "Layer: roberta_prelayernorm.encoder.layer.0.output\n",
      "Label smoothing avg cosine sim: 0.3341185946843692\n",
      "Layer: roberta_prelayernorm.encoder.layer.1.output\n",
      "Label smoothing avg cosine sim: 0.4312800140037284\n",
      "Layer: roberta_prelayernorm.encoder.layer.2.output\n",
      "Label smoothing avg cosine sim: 0.450046625044141\n",
      "Layer: roberta_prelayernorm.encoder.layer.3.output\n",
      "Label smoothing avg cosine sim: 0.4397071760880959\n",
      "Layer: roberta_prelayernorm.encoder.layer.4.output\n",
      "Label smoothing avg cosine sim: 0.4455120164257187\n",
      "Layer: roberta_prelayernorm.encoder.layer.5.output\n",
      "Label smoothing avg cosine sim: 0.4030209180120226\n",
      "Layer: roberta_prelayernorm.encoder.layer.6.output\n",
      "Label smoothing avg cosine sim: 0.3860304162918509\n",
      "Layer: roberta_prelayernorm.encoder.layer.7.output\n",
      "Label smoothing avg cosine sim: 0.3335717300792317\n"
     ]
    }
   ],
   "source": [
    "print(\"LABEL SMOOTHING RESULTS\")\n",
    "for layer_name in label_smoothing_anisotropy_computation.cosine_sims.keys():\n",
    "    # average the cosine similarities\n",
    "    label_smoothing_cosine_sims = label_smoothing_anisotropy_computation.cosine_sims[layer_name]\n",
    "    label_smoothing_avg_cosine_sim = sum(label_smoothing_cosine_sims) / len(label_smoothing_cosine_sims)\n",
    "\n",
    "    print(f\"Layer: {layer_name}\")\n",
    "    print(f\"Label smoothing avg cosine sim: {label_smoothing_avg_cosine_sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       " array([-0.16642827, -0.06642827,  0.03357173,  0.13357173,  0.23357173,\n",
       "         0.33357173,  0.43357173,  0.53357173,  0.63357173,  0.73357173,\n",
       "         0.83357173]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot out the cosine_sim on a plot \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(label_smoothing_avg_cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing Cosine Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out cosine sims\n",
    "import pickle\n",
    "\n",
    "with open(\"baseline_cosine_sims.pkl\", \"wb\") as f:\n",
    "    pickle.dump(baseline_anisotropy_computation.cosine_sims, f)\n",
    "\n",
    "with open(\"pos_smoothing_cosine_sims.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pos_smoothing_anisotropy_computation.cosine_sims, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the first 254 elements from the pos_smoothing_avg_cosine_sim \n",
    "\n",
    "pos_smoothing_avg_cosine_sim = {k: v[-53277:] for k, v in pos_smoothing_anisotropy_computation.cosine_sims.items()}\n",
    "pos_smoothing_anisotropy_computation.cosine_sims = pos_smoothing_avg_cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53277\n",
      "53277\n",
      "Layer: roberta_prelayernorm.encoder.layer.0.output\n",
      "Baseline avg cosine sim: 0.3433568946595581\n",
      "Pos smoothing avg cosine sim: 0.39338837753643396\n",
      "53277\n",
      "53277\n",
      "Layer: roberta_prelayernorm.encoder.layer.1.output\n",
      "Baseline avg cosine sim: 0.5185323326796372\n",
      "Pos smoothing avg cosine sim: 0.4872826279238394\n",
      "53277\n",
      "53277\n",
      "Layer: roberta_prelayernorm.encoder.layer.2.output\n",
      "Baseline avg cosine sim: 0.5281436800235679\n",
      "Pos smoothing avg cosine sim: 0.5059412399081986\n",
      "53277\n",
      "53277\n",
      "Layer: roberta_prelayernorm.encoder.layer.3.output\n",
      "Baseline avg cosine sim: 0.5132462359630755\n",
      "Pos smoothing avg cosine sim: 0.512955156903789\n",
      "53277\n",
      "53277\n",
      "Layer: roberta_prelayernorm.encoder.layer.4.output\n",
      "Baseline avg cosine sim: 0.519649225508813\n",
      "Pos smoothing avg cosine sim: 0.5076678278069083\n",
      "53277\n",
      "53277\n",
      "Layer: roberta_prelayernorm.encoder.layer.5.output\n",
      "Baseline avg cosine sim: 0.5047696811129256\n",
      "Pos smoothing avg cosine sim: 0.5285960882678392\n",
      "53277\n",
      "53277\n",
      "Layer: roberta_prelayernorm.encoder.layer.6.output\n",
      "Baseline avg cosine sim: 0.5201953046251375\n",
      "Pos smoothing avg cosine sim: 0.4904695875650569\n",
      "53277\n",
      "53277\n",
      "Layer: roberta_prelayernorm.encoder.layer.7.output\n",
      "Baseline avg cosine sim: 0.4949028764975487\n",
      "Pos smoothing avg cosine sim: 0.4748728620887101\n"
     ]
    }
   ],
   "source": [
    "for layer_name in pos_smoothing_anisotropy_computation.cosine_sims.keys():\n",
    "    # average the cosine similarities\n",
    "    baseline_cosine_sims = baseline_anisotropy_computation.cosine_sims[layer_name]\n",
    "    pos_smoothing_cosine_sims = pos_smoothing_anisotropy_computation.cosine_sims[layer_name]\n",
    "\n",
    "    print(len(baseline_cosine_sims))\n",
    "    print(len(pos_smoothing_cosine_sims))\n",
    "\n",
    "    baseline_avg_cosine_sim = sum(baseline_cosine_sims) / len(baseline_cosine_sims)\n",
    "    pos_smoothing_avg_cosine_sim = sum(pos_smoothing_cosine_sims) / len(pos_smoothing_cosine_sims)\n",
    "\n",
    "    print(f\"Layer: {layer_name}\")\n",
    "    print(f\"Baseline avg cosine sim: {baseline_avg_cosine_sim}\")\n",
    "    print(f\"Pos smoothing avg cosine sim: {pos_smoothing_avg_cosine_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "k# save out cosine sims\n",
    "import pickle\n",
    "\n",
    "with open(\"baseline_cosine_sims.pkl\", \"wb\") as f:\n",
    "    pickle.dump(baseline_anisotropy_computation.cosine_sims, f)\n",
    "\n",
    "with open(\"pos_smoothing_cosine_sims.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pos_smoothing_anisotropy_computation.cosine_sims, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pos-merge-Cf-JYAyy-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
